complexity hotspots
---

the worst feeling as a user of a general purpose computer is the sense that
the computer is getting away from you; the sense that you're no longer in
control of what's going on; you can't fathom why and don't even understand
where you would start to try to.

I want to be less susceptible to this feeling; I want to be empowered by
the technology I use and never get bogged down by breakage in something I can't
fix. I want to have a sense of mastery over and comfort in the tools that I
have to use to get my job done.

One of the tools I use frequently is Vim (a tremendously complex program, but
we'll get to that later). I find the relative simplicity of my Vim setup
comforting compared to that of an IDE or more complex editor. In it, the
simplest and most general way to navigate the filesystem is by
"fuzzy-searching" files, often scoped to the directory that Vim was launched
in. This functionality isn't built in to Vim, but it's popular enough that
there are many plugins that purport to provide it. 

The other day, I launched Vim in one of my project directories and typed in
"layout.css" to open the eponymously named file in the root of the project.
Bafflingly, my fuzzy search plugin reported "no results." This wasn't
complicated substring matching spread across word and path boundaries; this
was a failed match of a literal text string. Okay, I'm a programmer, and this
program is misbehaving... let's fix it. I jumped to the source of the plugin,
opened it up to begin debugging, and... wow. I just remembered that a) I don't
know vimscript and b) vimscript is an ancient, often inscrutable language,
full of questionable abbreviations and syntax that seemingly grew by
accretion. To add insult to injury, the plugin is **three thousand lines** of
the stuff... even if I knew the language, could I track down the bug in a
reasonable amount of time?

Thoroughly daunted, I gave up, but this episode stuck with me.

After a few weeks of this frustration gnawing at me, I reset my dev
environment. When something breaks in my computer system, I want the freedom
to fix it as simply and painlessly as possible. I want to be able to read the
source to the tools I use day to day and fix bugs or inconsistencies in their
behavior.  

The more complicated a tool is, the less of a chance I give myself to
comprehend or repair it when it breaks.
What aspects of my toolchain would I be comfortable eliminating or "trading
down" in complexity for simpler tools?

Here's some software I rely on to build programs day-to-day:

Mac OS X: closed-source, 87 million+ lines of code
Terminal.app: closed-source, unknown complexity
Safari: Webkit alone is 1.8 million lines of code
vim: 340485
git: 355018
fish: 47866
ag: 4329
selecta: 818
v8: 523000
node: 27687
scala: 172017

Here's a hypothetical list of ways I could "trade down":

linux: 15 million lines of code (jeeeez. still an order-of-magnitude reduction vs OS X.)
firefox: 12 million lines of code
mesa: a million lines of code
vim: 340485
git: 355018
btrfs: 86000 
weston: 28648
cljs: 23934 (a replacement for scala and ruby, though I can't evade the cost of a javascript runtime)
dash: 13485
pacman: (todo: cloc this later)
ag: 4329
st: 3594
selecta: 818

  note: many of the older C programs seem to also implement their own malloc and realloc.

Where are the hotspots in this profile?

The kernel & the web browser are both on the order of 10^7
the graphics system and programming vm => 10^6
editing and version controlling files => 10^5
the filesystem and window compositor => 10^4
the programming language and shell => a smaller 10^4
fast file search, terminal emulator => 10^3
fuzzy text selection => 10^2 (ruby)

Here's the thing: I don't think editing and version controlling files 
should be in that 10^5 tier. What's a reasonable lower bound for the
complexity of these tasks?

Well, Mary wrote gitlet.js as an exploration of the complexity of git. It's a
svelte 977 lines of javascript, back in the same ballpark as selecta.  And as
far as text editing is concerned, the simplest text editor that I know of is
ed, the standard unix editor: 1545 lines of C. Let's slot these in as lower
bounds. What does our profile look like now?

The kernel and web browser => 10^7
the graphics system and programming vm => 10^6
the filesystem, window compositor, shell and programming language => 10^5
search, terminal emulator, fuzzy selecting, editing, and version controlling text => 10^4

So let's try and split these programs up into categories. Where is most of our
complexity budget spent? Does the distribution make sense to us?

programmer interface:
  dash: 13485 (C)
  selecta: 818 (Ruby)
  ag: 4329 (C)
  find: 1803 (C)
  gitlet: 958 (js)
  ed: 1534 (C)

programming system:
  cljs: 23934 (cljs)
  v8, spidermonkey, or jsc: 292000-520000 (C++)

user interface:
  st: 3594 (C)
  weston: 28648 (C)
  mesa: 1000000 (C)
  webkit: 2 million (C++)

durability, persistence:
  btrfs: 86207 (C)

Giant chunks of this are related to performant implementations of javascript
and the web. It's sobering to realize that part of the legacy of ncsa mosaic
could be this giant complexity burden from html, css, and javascript.

Wait a second, though: how are we ever going to compile all of this C?
Compiler infrastructure is a grim story. Clang and LLVM are the
state-of-the-art compiler and compiler infrastructure, but they're massive;
LLVM is 680000 lines of code and clang is another 600000 on top of that.
There's TinyCC for C which is 61103 lines of code, in that same 10^5 ballpark.
It seems like a reasonable size for an optimizing compiler and assembler, but
doesn't support C++. LLVM is a requirement for Rust, too.

### The architecture of my programming environment: a hypothetical blog post series

1. Ref. [Linux Kernel Initialization](http://0xax.gitbooks.io/linux-insides/content/index.html)
2. Write up the architecture of the dash shell
3. Write up the architecture of TinyCC.
3. Write up the architecture of ed.
4. Write up the architecture of find.
5. Geoff Greer's writeups on the architecture of [ag](http://geoff.greer.fm/ag/)
6. Ref. the source code of [selecta](https://github.com/garybernhardt/selecta/blob/master/selecta)
7. Ref. David Nolen's [tour of the cljs compiler](https://github.com/swannodette/hello-cljsc/blob/master/src/hello_cljsc/core.clj)
8. Ref. Mary Rose Cook's [annotated gitlet source](http://gitlet.maryrosecook.com/docs/gitlet.html)
9. Ref. Julia Evans' writeup on the architecture of sqlite ([part 1](http://jvns.ca/blog/2014/09/27/how-does-sqlite-work-part-1-pages/), [part 2](http://jvns.ca/blog/2014/10/02/how-does-sqlite-work-part-2-btrees/))
10. Ref. Tonsky's writeup on [Datascript internals](http://tonsky.me/blog/datascript-internals/)

At this point we've got a terminal-only development environment. Wait, crap, I
forgot to include a javascript VM. I feel like the most reasonable thing to do
is include one of the big three VMs, but I'm also tempted by order-of-magnitude
smaller codebases like duktape (45kloc). I'm gonna benchmark clojurescript on
duktape for fun later.

Here's an interesting note. There are a few minimal but complete language
implementations I could potentially include: TinyCC (35k)  and Duktape (45k).

In the case of TinyCC and Duktape's, their bigger sibling language
implementations are at least an order of magnitude bigger (Clang is 680k,
jsc-v8 are between 250 and 500k). Do optimizations of a language necessitate an
order of magnitude increase in codebase size?

Anyway, moving on to the graphics stack:

11. Ref. the architecture of wayland
12. Write up the architecture of weston
13. Write up the architecture of st

There are two bigger monsters underlying the graphics stack: the drivers for
a particular graphics architecture (Intel's, for the sake of the conversation,
as it's fully open source), and Mesa, the linux graphics API.

The Intel graphics codebase contains support for many different wholly separate
codepaths for each graphics card. It would be interesting to try and see the
graph of dependencies within the driver to find out how big the codepath 
actually used by any one particular driver is; I have a hunch that this would
drop the codebase to a surmountable size that I could actually comprehend.

I have a hunch that a similar trick could be played with the Mesa codebase, but
haven't done the investigative work required to verify.

Let's revisit some source lines of code counts:

dash: 13485 (C)
bsd find: 1803 (C)
selecta: 818 (Ruby)
ed: 1534
ag: 4329 (C)
cljs: 23934 (cljs)
gitlet: 958 (js)
weston: 28648 (C)
st: 3594 (C)

I guess application-level persistence is a separate concern from the rest of the
programming environment; none of the aforementioned programs use sqlite or anything
similar. 

datascript: 866 (cljs)
sqlite: 94200 (C)

(eventually)
servo: 200000 (rust)
browser.html: 2590 (js)

I removed rsync, because I wasn't sure what job it did that the combination of
http/socket transport and checksumming didn't. It might not be worth its
complexity cost. Ah, speaking of checksumming: we probably need to include
md5 or the like somewhere in here, though it might just be a library function.
Compression, too: gzip is critical to many software infrastructures.

I eliminated rlwrap from this list because dash takes a -E flag for emacs style
interactive edits; we're already paying the complexity cost for that and we
don't need to pay it twice. We could also remove interactive edit functionality
from dash, using rlwrap exclusively, though I observed some strange redraw and
job control problems when using `rlwrap dash` together as a login shell.

Ah, another mistake caught: I forgot to include the Node source code itself; 
v8 and spidermonkey might not be enough on their own to reliably script the
system. Then again, who knows? I haven't tried it. Let's check the size of the
node source:

node: 27687 (C++)

Okay, so a similar cost to the other language runtimes we've seen, but it
affords us the freedom to do platform-agnostic io and system calls. I think
this one is probably worth its complexity cost, because I'm not sure what
scripting unix with the bare v8 or spidermonkey API would be like.

Then again, consider the duktape interpreter: designed to be easily embeddable
and scriptable from the rest of the system. Maybe designing a simple system
call layer accessible from cljs wouldn't be such a bad thing if it saved us 27k
lines of C++ as a dependency? The flip side of this is the fact that node is
relatively battle-tested at this point. Do I trust myself to write a solid
system call layer?

Actually, maybe I do. `#include syscall.h` in a C program that reads JavaScript
source from the filesystem isn't such a high price to pay for comprehensibility
and the freedom to build higher-level abstractions on top of the syscalls I have
in ClojureScript. Maybe I could even build a separate API for myself that trades
off control for simplicity and reliability, along the lines of "rethinking the
Unix filesystem API" from earlier.

Okay, let's take that (wildly optimistic) last paragraph into account and do some
rudimentary addition.

tinycc: 61103 (C)
dash: 13485 (C)
find: 1803 (C)
selecta: 818 (Ruby)
ed: 1534
ag: 4329 (C)
duktape: 45319 (C)
cljs: 23934 (cljs)
gitlet: 958 (js)
weston: 28648 (C)
st: 3594 (C)

total: 185525

Maybe we're getting into tenable territory?  Maybe tinycc and duktape are below
the level at which I should be thinking. If that's the case, I could easily
slot in Clang and node, though I remain agnostic for now. Sans those two, we're
at a completely manageable 79,103 lines of code, not counting tests or docs for
those codebases that have them.

I see a lot of promise here. There's stranger complications (how are we going to
handle the web?) but perhaps this is a good place to stop and think.

(eventually)
servo: 200000 (rust)
browser.html: 2590 (js)
