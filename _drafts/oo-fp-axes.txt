two kinds

"Object-oriented" and "functional" programming styles have often been cast as
opposing, but "polymorphism and late-binding" (the definition of
object-oriented programming) does not preclude "programming without side
effects" (the definition of functional programming). How do these styles of
program design complement each other in the small and in the large?


Dijkstra on the origins of hierarchy:

> I do not know of any other technology covering a ratio of 10^10 or more: the
> computer, by virtue of its fantastic speed, seems to be the first to provide
> us with an environment where highly hierarchical artefacts are both possible
> and necessary. 

- The Humble Programmer, 1972 
https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html

Hickey (abridged) on the origins of mutability:

> There's a reason we do Place-Oriented Programming because way back in the
> early days of computers, we had to do Place-Oriented Programming. There
> wasn't enough capacity to do anything else. In the time that I've been doing
> programming, the capacity of these two things (RAM and disk) have increased a
> millionfold. Imagine if your car was a million times bigger than it is.
> What rules would still apply?  What characteristics would still be true?
> Almost nothing but yet, we're retaining decisions we made when things were
> much much smaller and moving forward with it.

- The Value of Values, 2012 
https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/ValueOfValues.md


thesis: the exponential increase in the speed and memory capacity of computers
enables immutability / referential transparency, which enable less hierarchical
programming artefacts

when programming began, we had to do place-oriented programming - shared
mutable state was essential. it's no longer essential. (anecdote? graph of
transistor density & price?)

note: "mutability" semantics are dependent on your vantage point.
for the callers of a procedure with no side effects, it doesn't matter if we
allocate a buffer and write bytes into it. we can use the high level example
of the computing device itself

hiding the state of a procedure, "encapsulation" or "information hiding", was
critical to reasoning about extremely stateful systems - a way of reducing the
amount of state sharing between parent and child nodes
  (the Design of Systems Programs - book 1970, and 'on the criteria to be used
   in decomposing systems into modules' - paper 1972)

now we have a choice: do we share? do we mutate? when to do which?

we can use immutable data structures and copy-on-write procedures to share
without mutating. mutating without sharing doesn't break referential
transparency, so the procedures can still be considered pure. 

for sharing without mutating or mutating without sharing, you have referential
transparency, which implies:
=> immutability (copy-on-write procedures)
   - why? sharing without mutating requires it
=> parallel (without necessitating concurrent) abstractions
   - cf. fork-join
   - cf. par monad + spark pools
   - why? procedures that are referentially transparent don't care how they're
     called, don't need to cooperate or coordinate with other procedures for 
     invocation. coordination is expensive in terms of computer time (slow) 
     and programmer time (hard to understand).
=> large, general, transparent interfaces with lots of surface area 
   - cf clojure collections
   - cf scipy, numpy
=> early-binding, static, "library" code
   - doesn't matter whether we AOT - don't need to make many runtime decisions
=> amenable to static analyses (type-driven)
=> stateless unit testing
  - cf quickcheck
  - cf boundaries
  - talk about linear / superlinear test suite runtime growth
=> very little high-level organization required
  - cf Joe Armstrong's intuition on global scope for all functions
=> encapsulation isn't as important
  - cf Hoogle

when mutating *and* sharing, though, you need to think about the semantics of a
shared mutable reference - you're back to the principles that guided us during
place-orientation:

shared mutable reference =>
=> small, specific, opaque interfaces with little surface area (cf posix file api?) 
   - why? help us reason about code - fewer places the state comes from
     - examples of oo principles serving this goal
=> concurrent (as opposed to parallel) abstractions 
   - why? heterogeneity and doing many different things at once - each state-bundle
     can require a different procedure to manage it - concurrency is about coordinating
     heterogenous processes
=> integration testing 
   - why? the complexity lies in the interleaving / overlaying of many procedures onto
     the same data - you must test that interleaving
   - talk about 2^N state space explosion
=> "framework" code
   - why? the integration of many different parts is extremely difficult at scale because
     of the effects of state - it's more effective to try and get the high-level organization
     right once (remember, the shape of the interleaving is key to the success of the design!)
     and allow for arbitrary code that fulfills a contract to run at the leaves of the code
     tree
=> late-binding
   - why? we want to get high-level organization right once and not spend the inordinate resources
     getting it right again - we want the organization code to be able to call the 'purposeful'
     code without knowing anything specific about what kind of 'purposeful' code we'll write
=> dynamism
   - why? enabling late-binding
     cf. "why objects are inevitable": objects were inevitable because having
     some structured way of doing dynamism, indirection & late binding is
     necessary for building frameworks
=> needs interactive development (inspection of state, repl-driven, live-coding)
   - why? state progression is key, the state space is huge, ad-hoc reasoning and
     observation needs to be cheap!
=> high-level organization and hierarchy is critical (program organization is
   treelike w/ low branching factor)
   - why? it's necessary to reason about the whole thing together, so breaking
     parts of it up into smaller pieces with agreed-upon interfaces gives you
     some hope of reasoning about parts
=> encapsulation is critical
   - why? it's necessary to understand the whole system - any subtree can break
     an invariant that's relied on by any other subtree - hopefully not but it's
     possible when it's mutating its parent's scope


mutable reference semantics boil down to scope control. there's data from the
outside world (relative to the procedure or program you're writing) whose scope
you don't control, and mutations to which you must defend against. 

as shared mutable state programmers, we know this! 
ex. pervasive defensive copying

oo is about interfaces and organization - late-binding meaning "the invocation
of a procedure without knowing what code will be called at runtime". whether
explicit (Java) or implicit (Python) the idea of having a common "interface" to
a group of "objects" is key to making this work.

very, very interesting that all of Clojure's (and Lisp/Smalltalk more
generally) polymorphism + late binding happens in the language implementation
to permit extensibility 
- 'how does procedure invocation happen?' 
- 'what is a function?'
- 'what is permitted in the syntax' 
- 'what control structures can we express'

This enables Clojure's 'frameworkless web development' - this is broadly true of
other dynamic languages too (cf. frameworkless javascript)

vs. C/C++/Java where polymorphism + late binding isn't how the language is implemented
